{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aims:\n",
    "1. Compare the FFNN code with logistic regression. Specifically, compare the neural network classification results with the results obtained using another method(?). Use the Wisconsin Breast Cancer data to save time.\n",
    "2. Define the cost function and design matrix first.\n",
    "3. Write a logistic regression code using the SGD algorithm (from the FFNN code?). You can also use standard gradient descent in this case, with a learning rate as hyperparameter (only if time).\n",
    "4. Study the results as functions of the chosen learning rates.\n",
    "5. Add also an L2 regularization paramater $\\lambda$ (like with Ridge regression). Compare these results with those from the FFNN code as well as those obtained using scikit-learnâ€™s logistic regression functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO what is the point of this? the FFNN code can perform logistic regression?\n",
    "#TODO use the AND/OR/XOR gates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO example from Week 42, using the AND gate\n",
    "\n",
    "import numpy as np\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.beta_logreg = None\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    def GDfit(self, X, y):\n",
    "        n_data, num_features = X.shape\n",
    "        self.beta_logreg = np.zeros(num_features)\n",
    "        for _ in range(self.num_iterations):\n",
    "            linear_model = X @ self.beta_logreg\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "            # Gradient calculation\n",
    "            gradient = (X.T @ (y_predicted - y))/n_data\n",
    "            # Update beta_logreg\n",
    "            self.beta_logreg -= self.learning_rate*gradient\n",
    "    def predict(self, X):\n",
    "        linear_model = X @ self.beta_logreg\n",
    "        y_predicted = self.sigmoid(linear_model)\n",
    "        return [1 if i >= 0.5 else 0 for i in y_predicted]\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data\n",
    "    X = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])\n",
    "    y = np.array([0, 0, 0, 1])  # This is an AND gate\n",
    "    model = LogisticRegression(learning_rate=0.01, num_iterations=1000)\n",
    "    model.GDfit(X, y)\n",
    "    predictions = model.predict(X)\n",
    "    print(\"Predictions:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
