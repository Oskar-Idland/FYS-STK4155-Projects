
\subsection{Regression Analysis}
To explore the optimization methods discussed in the theory \cref{sec:theory}, we first consider a regression problem using the Franke function. This function is a widely used benchmark for regression tasks, as it provides a smooth, non-linear surface that can be sampled to generate noisy data. We generate a dataset by sampling the Franke function with added Gaussian noise, which we then use to train and evaluate our regression models. We fit our model to this data with a polynomial design matrix, after splitting it into training and test sets, and evaluate the model's performance using metrics like the mean squared error (MSE) and \( R^2 \) score.

Regression models have many parameters, making it impossible (or at least impractical) to iterate over all possible combinations. We use grid search to explore two parameters at a time. We then zoom in on the most promising regions to find the optimal parameters. After finding a suitable set of parameters, we continue the search for the next set of parameters.

\subsubsection{Plain Gradient Descent}
\paragraph*{No Momentum:}
To be sure we understand the basic principles of gradient descent, we start by implementing a simple gradient descent algorithm for linear regression. We use the MSE as our cost function and compute the gradient of the cost function with respect to the model parameters. We then update the parameters using the gradient and iterating over a set of learning rates.
\paragraph*{Momentum:}
We then extend our implementation to include momentum. Now we iterate over all possible values of the momentum and learning rate to find the optimal combination for our model. After creating a general overview of the entire parameter space, we zoom in on the most promising regions to find the optimal parameters. This process will be repeated for the other optimization methods as well.

\subsubsection{Stochastic Gradient Descent}
\paragraph*{No Momentum:}
Continuing with stochastic gradient descent, we implement a simple version of the algorithm and apply it to our regression problem. We then begin by iterating over all possible learning rates and batch sizes to find the optimal combination. We then zoom in on the most promising regions to find the optimal parameters.
\paragraph*{Momentum:}
Secondly, we use the ideal learning rate, and then iterate over possible values of the momentum to find the optimal combination. We then zoom in on the most promising regions to find the optimal parameters.

Moving on to the more advanced optimization methods, we implement AdaGrad, RMSprop, and Adam. We follow the same procedure as before, iterating over all possible learning rates and momentum values to find the optimal combination. We note the optimal values to use in the neural network implementation.

\begin{comment}
  #TODO: Update if we iterate over momentum and rho values as well
\end{comment}

\subsubsection{AdaGrad}
\paragraph{No Momentum:}
For both the regular and stochastic versions of AdaGrad, we iterate over all possible learning rates to find the optimal value.

\paragraph{Momentum:}
Using the optimal values found in the previous step, we then iterate over all possible values of the momentum to find the optimal value to pair with the learning rate.

\subsubsection{RMSprop \& Adam}
\paragraph{No Momentum:}

\subsection{Neural Networks}
In order to explore the optimization methods discussed in the theory section, we consider again a regression problem on the Franke function, and also a classification problem on the Wisconsin Breast Cancer dataset. % #TODO cite sklearn
The hyperparameter space is vast, and in order to effectively produce results we narrow down the possibilities by first deciding on a singular network model to base our work on. This entails initializing our models with pseudo-random weights sampled from a univariate normal distribution with zero mean and a variance of one. The same goes for the biases, but with an added constant of 0.01 in order to break symmetry. With the focus on exploring the influence of hyperparameters, we choose to keep the network architecture simple, with a single hidden layer and a fixed number of nodes. This allows us to focus on the effects of the learning rate, batch size, and regularization strength on the network's performance.
\subsubsection{Franke Function Regression}

We first validated our neural network implementation using the Franke function regression problem. Input data was generated by sampling the Franke function on a uniform grid of \( 100 \times 100 \) points over the unit square \( [0,1]\times[0,1] \). To simulate measurement noise, we added Gaussian noise with mean zero and standard deviation \( \sigma = 0.01 \) to the function values.

The input coordinates \( (x,y) \) were used to generate a design matrix with polynomial features up to degree 4, chosen to match the complexity of the Franke function's structure. The dataset was split into training \( (80\%) \) and test \( (20\%) \) sets using scikit-learn's \verb|train_test_split function|. Both input features and target values were standardized using StandardScaler, with the scaling parameters computed only from the training set to prevent data leakage.

The network was configured with a single hidden layer of 15 nodes, empirically chosen as a balance between model complexity and computational efficiency. The output layer consisted of a single node for regression. We initialized weights from a normal distribution \( \mathcal{N} (0,1) \) and biases with a small constant offset of 0.01 to break symmetry in the network's initial state.

For our systematic hyperparameter exploration, we established a standard configuration:
\begin{itemize}
    \item Activation function: Sigmoid in the hidden layer
    \item Cost function: Mean Squared Error (MSE)
    \item Optimizer: AdaGrad with momentum (\( \eta \) = 0.01, \( \gamma \) = 0.8)
    \item Regularization rate: \( \lambda \) = 0.0001
\end{itemize}

These baseline parameters were chosen based on preliminary experiments showing stable convergence without excessive overfitting. This configuration served as our control point, allowing us to systematically vary individual parameters while holding others constant.

\paragraph*{Epochs vs batch size:}
Our first step was to explore the relationship between the number of epochs and the batch size. We iterated over epoch sizes \( (100, 500, 1000, 2000) \) and number of batches \( (1, 10, 20, 50, 100) \) and calculated the score metrics for each combination. From here we went further with using 500 epochs and 20 batches for the rest of the analysis.

\paragraph*{Learning rate (\( \eta \)) vs Regularization (\( \lambda \)):}
This was conducted in similar fashion to the previous step, with the learning rate \( \eta \) ranging from values (\( 1e^{-4} \) to \( 1e^{-1} \) and 0.5), and the regularization parameter \( \lambda \) ranging from (\( 1e^{-5} \) to \( 1e^{-1} \)).

\paragraph*{Schedulers:}\label{subsec:nn_schedulers}
To compare the different learning rate schedulers discussed in \cref{sec:learning_rate_tuning}, we implemented one instance of each of them with the standard learning rate. For plain momentum and Adagrad with momentum, we used the parameters \( \gamma = 0.8 \) again. For Adam, we used the parameters \( \rho = 0.9, \rho_2 = 0.999 \). RMPprop had the same \( \rho \) value as Adam. To ensure numerical stability, we used a small value of \( \epsilon = 1e^{-8} \) for all the optimizers. The momentum and decay parameters were chosen based on common values in deep learning literature that typically show good performance across different problems. % #TODO cite

Training progression was monitored by computing both MSE and \( R^2 \) scores on the training data at each epoch. Convergence was defined quantitatively as the point where the relative difference between the mean MSE of the last 10 epochs and the current MSE fell below \( 10^{-2} \), providing a relative convergence criterion for comparing optimizer efficiency. For final model evaluation, we computed these metrics on the held-out test set to assess generalization performance.
\subsubsection{Breast Cancer Classification}

For the classification task, we used the Wisconsin Breast Cancer dataset, which contains 30 features computed from digitized images of fine needle aspirates (FNA) of breast mass, with binary labels indicating malignant or benign tumors. The data was split into training and test sets using an 80-20 split. Unlike the Franke function data, only the input features were standardized since the target values are binary.

We maintained the same basic network architecture as in the regression task, initially using a single hidden layer with 15 nodes. The sigmoid activation function was employed in both the hidden and output layers, with the output layer producing a single value representing the probability of malignancy. For this binary classification problem, we used the logistic regression cost function \cref{logistic_cost}.

Given the relatively small size of the dataset compared to the Franke function case, we reduced the training duration to 20 epochs with 20 batches. The initial configuration used a regularization parameter \( \lambda \) = 0.01 and the Adam optimizer with learning rate \( \eta \) = 0.001, and momentum parameters \( \rho \) = 0.9, \( \rho_2 \) = 0.999. Model performance was evaluated using both training and test set accuracy scores.

Our investigation proceeded in three main stages:

\paragraph*{Learning Rate and Regularization Parameter:}
Learning Rate and Regularization: We performed a grid search over learning rates and regularization parameters, both ranging from \( 10^{-5} \) to \( 10^{-1} \). This explored the balance between model convergence speed and stability versus overfitting prevention.

\paragraph*{Network Architecture:}
We systematically investigated the effect of network depth and width by varying:
\begin{itemize}
    \item Number of hidden layers: 1 to 3 layers
    \item Neurons per layer: 5 to 25 neurons, in steps of 5
\end{itemize}
Each configuration was trained with the previously determined optimal learning rate and regularization parameter to isolate the effect of architecture changes.

\paragraph*{Activation Functions:}
Maintaining the 15-neuron architecture, we compared three different activation functions in the hidden layers:
\begin{itemize}
    \item Sigmoid
    \item ReLU
    \item Leaky ReLU
\end{itemize}
This comparison was performed across networks with 1 to 3 hidden layers to understand how different activation functions affect the network's learning capacity at varying depths. The output layer retained the sigmoid activation function to maintain proper probability outputs for classification.

\subsection{Logistic Regression}
To evaluate different approaches to classification of the Wisconsin Breast Cancer dataset, we implemented logistic regression alongside our neural network. While our neural network used a single hidden layer, logistic regression represents an even simpler model that can be viewed as a neural network without any hidden layers, using only a sigmoid activation function on the output.

Using the same preprocessed dataset as in our neural network analysis (standardized features, 80-20 train-test split), we implemented logistic regression with the standard sigmoid function and logistic regression cost function.

To enable direct comparison with our neural network results, we performed a similar grid search over:

\begin{itemize}
    \item Learning rates \( \eta \): [\( 10^-{5,} 10^{-4}, 10{^-}3, 10^{-2}, 10^{-1} \)]
    \item Regularization \( \lambda \): [\( 10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1} \)]
\end{itemize}
We maintained consistency with our other implementations by using stochastic gradient descent with batch size 20 and 20 epochs. The model's performance was evaluated using the accuracy score on both training and test sets, allowing comparison between the simpler logistic regression and the single-hidden-layer neural network approach.

\subsection{Benchmark Implementations}
For validation and comparison purposes, we implemented equivalent models using established machine learning frameworks - PyTorch\cite{PyTorch} and scikit-learn\cite{scikit-learn}. This allowed us to benchmark our implementations against industry-standard tools.

For the Franke function regression problem, we created a PyTorch neural network with a similar architecture to our implementation: an input layer of 2 neurons (for x and y coordinates), two hidden layers of 50 and 25 neurons with sigmoid activation functions, and a single output neuron with linear activation. The model was trained using stochastic gradient descent with a batch size of 32 over 2000 epochs.

For the breast cancer classification task, we implemented both a PyTorch neural network and scikit-learn's LogisticRegression. The PyTorch model consisted of two hidden layers (10 and 5 neurons) with sigmoid activations, followed by a binary output layer. For direct comparison with our logistic regression implementation, we utilized scikit-learn's implementation with L2 regularization and the LBFGS optimizer. Both frameworks' models were trained on the same standardized data splits as our implementations to ensure fair comparison.

To maintain consistency in evaluation metrics, we used the same performance measures (MSE and $R^2$ for regression, accuracy for classification) across all implementations. This provided a robust framework for validating our custom implementations against established tools.

\subsection{Program}\label{sec:program}
\subsubsection{Code Structure}\label{subsec:codestructure}
All the source code that we developed and used to produce our results is available on our GitHub repository, linked in \cref{appsec:code} in \verb|Project_2/src|. The \verb|README.md| file contains the entire project file structure. To replicate our exact results, use the provided \verb|requirements.txt| file to install the necessary packages. Our code is divided into the following files and notebooks.

\paragraph*{Regression Analysis}
\verb|RegressionModel.py| contains the classes for the regression models.

\verb|regression_anal.ipynb| contains the analysis of the Franke function data, which we used to validate our implementation.

\paragraph*{Neural Networks}
\verb|FFNN.py| contains the classes for the neural networks.

\verb|nn_Franke.ipynb| contains the analysis of the Franke function to validate the implementation of neural networks.

\verb|nn_breast_cancer.ipynb| contains the analysis of the breast cancer data using neural networks. This is where we truly optimize the hyperparameters of the neural networks.

\verb|logistic_regression_anal.ipynb| contains the analysis of the breast cancer data using logistic regression.

\verb|nn_pytorch.ipynb| Contains the analysis of the Franke function and the Wisconsin Breast Cancer dataset using PyTorch and the analysis of the latter dataset using scikit-learn's logistic regression implementation.

\verb|activation_funcs.py| contains the activation functions used in the neural networks.

\verb|cost_funcs.py| contains the cost functions used in the neural networks.

\paragraph*{Miscellaneous}
\verb|utils.py| contains utility functions used throughout the project. This includes plotting, data generation and other repetitive tasks.

\subsection{Tools}\label{subsec:tools}

All our code is written in Python (3.12) \cite{Python},  and we used scikit-learn \cite{scikit-learn} and PyTorch\cite{PyTorch} to test against our models. To vectorize our code we used \verb|numpy| \cite{Numpy}, and for visualization we used \verb|matplotlib.pyplot| \cite{Matplotlib}. All python packages and their versions can be found in our \verb|requirements.txt|. Code completion and debugging was done in Visual Studio Code \cite{VSCode} with additional assistance of GitHub Copilot \cite{Copilot}. We used \verb|git| \cite{Git} for version control, and \verb|GitHub| \cite{GitHub} for remote storage of our code.
