\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{gasoline}
\newlabel{FirstPage}{{}{1}{}{section*.1}{}}
\newlabel{FirstPage@cref}{{}{[1][1][]1}}
\@writefile{toc}{\contentsline {title}{\begin{Large}Project 1\end{Large}\\\vspace  {5pt}FYS-STK4155}{1}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {abstract}{Abstract}{1}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section*.3}\protected@file@percent }
\newlabel{sec:introduction}{{I}{1}{}{section*.3}{}}
\newlabel{sec:introduction@cref}{{[section][1][]I}{[1][1][]1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Theory}{1}{section*.4}\protected@file@percent }
\newlabel{sec:theory}{{II}{1}{}{section*.4}{}}
\newlabel{sec:theory@cref}{{[section][2][]II}{[1][1][]1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Regression Analysis}{1}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Ordinary Least Squares}{2}{section*.6}\protected@file@percent }
\newlabel{subsubsec:ols}{{II\,A\,1}{2}{}{section*.6}{}}
\newlabel{subsubsec:ols@cref}{{[subsubsection][1][2,1]II\,A\,1}{[1][2][]2}}
\newlabel{eq:OLS cost}{{5}{2}{}{equation.2.5}{}}
\newlabel{eq:OLS cost@cref}{{[equation][5][]5}{[1][2][]2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Ridge}{2}{section*.7}\protected@file@percent }
\newlabel{subsubsec:ridge}{{II\,A\,2}{2}{}{section*.7}{}}
\newlabel{subsubsec:ridge@cref}{{[subsubsection][2][2,1]II\,A\,2}{[1][2][]2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}Lasso}{3}{section*.8}\protected@file@percent }
\newlabel{subsubsec:lasso}{{II\,A\,3}{3}{}{section*.8}{}}
\newlabel{subsubsec:lasso@cref}{{[subsubsection][3][2,1]II\,A\,3}{[1][3][]3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Properties of Predictive Models}{3}{section*.9}\protected@file@percent }
\newlabel{subsec:tradeoff}{{II\,B}{3}{}{section*.9}{}}
\newlabel{subsec:tradeoff@cref}{{[subsection][2][2]II\,B}{[1][3][]3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Predicted Values}{3}{section*.10}\protected@file@percent }
\newlabel{eq:Bias}{{16}{3}{}{equation.2.16}{}}
\newlabel{eq:Bias@cref}{{[equation][16][]16}{[1][3][]3}}
\newlabel{eq:Var}{{17}{3}{}{equation.2.17}{}}
\newlabel{eq:Var@cref}{{[equation][17][]17}{[1][3][]3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Model Bias}{3}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}Model Variance}{3}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4}Bias-Variance Tradeoff}{4}{section*.13}\protected@file@percent }
\newlabel{eq:MSE}{{18}{4}{}{equation.2.18}{}}
\newlabel{eq:MSE@cref}{{[equation][18][]18}{[1][4][]4}}
\newlabel{eq:R2}{{19}{4}{}{equation.2.19}{}}
\newlabel{eq:R2@cref}{{[equation][19][]19}{[1][4][]4}}
\newlabel{eq:model error}{{20}{4}{}{equation.2.20}{}}
\newlabel{eq:model error@cref}{{[equation][20][]20}{[1][4][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Resampling Methods}{4}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Bootstrap}{4}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Cross-Validation}{4}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D}The Franke Function}{4}{section*.17}\protected@file@percent }
\newlabel{subsec:franke}{{II\,D}{4}{}{section*.17}{}}
\newlabel{subsec:franke@cref}{{[subsection][4][2]II\,D}{[1][4][]4}}
\citation{gasoline}
\citation{music}
\newlabel{eq:Franke}{{{21}}{5}{}{AMS.19}{}}
\newlabel{eq:Franke@cref}{{[equation][2147483647][]{21}}{[1][4][]5}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Methods \& Implementation}{5}{section*.20}\protected@file@percent }
\newlabel{sec:methods}{{III}{5}{}{section*.20}{}}
\newlabel{sec:methods@cref}{{[section][3][]III}{[1][5][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Generating the Data}{5}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Synthetic Data}{5}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Cosmological Simulation Data}{5}{section*.24}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Visualization of the two-dimensional Franke function expressed in \eqref  {eq:Franke}.}}{5}{figure.caption.23}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Franke}{{1}{5}{Visualization of the two-dimensional Franke function expressed in \eqref {eq:Franke}}{figure.caption.23}{}}
\newlabel{fig:Franke@cref}{{[figure][1][]1}{[1][5][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Regression Analysis}{5}{section*.26}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Creating the Design Matrix}{5}{section*.27}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Logarithm of dark matter density in the simulation box averaged over one of the three axes at redshift $z\approx 12.88$. Plotted as contour plot on the left and as surface plot on the right. Because the Universe expands as time goes on the box does as well, and we therefore scale the coordinates of the box by multiplying with $a$. Similarly, space between the dark matter particles also increases with time, which is why the density is scaled by multiplying with $a^{-3}$. G\begin{footnotesize}ASOLINE\end{footnotesize}2 uses the rest of the scaling factors by default.}}{6}{figure.caption.25}\protected@file@percent }
\newlabel{fig:density}{{2}{6}{Logarithm of dark matter density in the simulation box averaged over one of the three axes at redshift $z\approx 12.88$. Plotted as contour plot on the left and as surface plot on the right. Because the Universe expands as time goes on the box does as well, and we therefore scale the coordinates of the box by multiplying with $a$. Similarly, space between the dark matter particles also increases with time, which is why the density is scaled by multiplying with $a^{-3}$. G\begin {footnotesize}ASOLINE\end {footnotesize}2 uses the rest of the scaling factors by default}{figure.caption.25}{}}
\newlabel{fig:density@cref}{{[figure][2][]2}{[1][5][]6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Preprocessing}{6}{section*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}Parameter Dependencies}{7}{section*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Bias-Variance Tradeoff}{7}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Bootstrap With OLS}{7}{section*.32}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Cross-Validation}{7}{section*.33}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Fig. 2.11 of Hastie, Tibshirani, and Friedman illustrating bias-variance trade-off as a function of model complexity \colorbox {magenta}{cite}}}{8}{figure.caption.31}\protected@file@percent }
\newlabel{fig:Hastie}{{3}{8}{Fig. 2.11 of Hastie, Tibshirani, and Friedman illustrating bias-variance trade-off as a function of model complexity \colorbox {magenta}{cite}}{figure.caption.31}{}}
\newlabel{fig:Hastie@cref}{{[figure][3][]3}{[1][7][]8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}The Program}{8}{section*.34}\protected@file@percent }
\newlabel{subsec:program}{{III\,D}{8}{}{section*.34}{}}
\newlabel{subsec:program@cref}{{[subsection][4][3]III\,D}{[1][7][]8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Code Structure}{8}{section*.35}\protected@file@percent }
\newlabel{subsubsec:codestructure}{{III\,D\,1}{8}{}{section*.35}{}}
\newlabel{subsubsec:codestructure@cref}{{[subsubsection][1][3,4]III\,D\,1}{[1][7][]8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Tools}{8}{section*.36}\protected@file@percent }
\newlabel{subsubsec:tools}{{III\,D\,2}{8}{}{section*.36}{}}
\newlabel{subsubsec:tools@cref}{{[subsubsection][2][3,4]III\,D\,2}{[1][7][]8}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Results \& Discussion}{8}{section*.37}\protected@file@percent }
\newlabel{sec:results discussion}{{IV}{8}{}{section*.37}{}}
\newlabel{sec:results discussion@cref}{{[section][4][]IV}{[1][8][]8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Effects of Data Scaling}{8}{section*.38}\protected@file@percent }
\newlabel{subsec:scaling effect}{{IV\,A}{8}{}{section*.38}{}}
\newlabel{subsec:scaling effect@cref}{{[subsection][1][4]IV\,A}{[1][8][]8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Hyperparameter Dependency}{8}{section*.42}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Ridge}{8}{section*.43}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Lasso}{8}{section*.45}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Bias-Variance Tradeoff}{8}{section*.47}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Resampling Methods}{8}{section*.50}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}OLS: Cross-Validation VS Bootstrap}{8}{section*.51}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Cross-Validation With Ridge and Lasso}{8}{section*.54}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {E}Fits to the Cosmological Data}{8}{section*.56}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Complexity Dependency for OLS}{8}{section*.57}\protected@file@percent }
\bibdata{Project_1Notes,references}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces MSE (upper row) and $R^2$ (lower row) evaluated on the Franke function test set using unscaled (left column) and scaled (right column) values, for polynomial degrees in the interval $[1,6]$.}}{9}{figure.caption.39}\protected@file@percent }
\newlabel{fig:a error scaled vs raw}{{4}{9}{MSE (upper row) and $R^2$ (lower row) evaluated on the Franke function test set using unscaled (left column) and scaled (right column) values, for polynomial degrees in the interval $[1,6]$}{figure.caption.39}{}}
\newlabel{fig:a error scaled vs raw@cref}{{[figure][4][]4}{[1][8][]9}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces MSE (upper row) and $R^2$ (lower row) evaluated on the cosmological test set using unscaled (left column) and scaled (right column) values, for polynomial degrees in the interval $[1,41]$.}}{9}{figure.caption.40}\protected@file@percent }
\newlabel{fig:g error scaled vs raw}{{5}{9}{MSE (upper row) and $R^2$ (lower row) evaluated on the cosmological test set using unscaled (left column) and scaled (right column) values, for polynomial degrees in the interval $[1,41]$}{figure.caption.40}{}}
\newlabel{fig:g error scaled vs raw@cref}{{[figure][5][]5}{[1][8][]9}}
\bibcite{gasoline}{{1}{2024}{{James W.~Wadsley}}{{}}}
\bibcite{music}{{2}{2024}{{Hahn}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The coefficients $\beta $ for the cosmological fit estimated using unscaled (purple) and scaled (orange) data, plotted for polynomial degrees in the interval $[1,41]$. We have used a logarithmic scale for the $\beta $'s.}}{10}{figure.caption.41}\protected@file@percent }
\newlabel{fig:g beta scaled vs raw}{{6}{10}{The coefficients $\beta $ for the cosmological fit estimated using unscaled (purple) and scaled (orange) data, plotted for polynomial degrees in the interval $[1,41]$. We have used a logarithmic scale for the $\beta $'s}{figure.caption.41}{}}
\newlabel{fig:g beta scaled vs raw@cref}{{[figure][6][]6}{[1][8][]10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Comparing Regression Variants}{10}{section*.59}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{10}{section*.61}\protected@file@percent }
\newlabel{sec:conclusion}{{V}{10}{}{section*.61}{}}
\newlabel{sec:conclusion@cref}{{[section][5][]V}{[1][9][]10}}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{10}{section*.62}\protected@file@percent }
\newlabel{LastBibItem}{{2}{10}{}{section*.62}{}}
\newlabel{LastBibItem@cref}{{[section][5][]V}{[1][10][]10}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces MSE (upper row) and $R^2$ (lower row) for Ridge regression evaluated on the Franke test data and contour plotted as functions of polynomial degree (horizontal axes) and hyperparameter $\lambda $ (vertical axes). We have used 21 evenly spaced degrees in the interval $[1, 41]$ and 30 logarithmically spaced values for $\lambda $ in the interval $[10^{-5},10^{-1}]$. We have cropped away degrees 1 and 3 and the 15 largest $\lambda $ values purely for visualization purposes.}}{11}{figure.caption.44}\protected@file@percent }
\newlabel{fig:b MSE R2}{{7}{11}{MSE (upper row) and $R^2$ (lower row) for Ridge regression evaluated on the Franke test data and contour plotted as functions of polynomial degree (horizontal axes) and hyperparameter $\lambda $ (vertical axes). We have used 21 evenly spaced degrees in the interval $[1, 41]$ and 30 logarithmically spaced values for $\lambda $ in the interval $[10^{-5},10^{-1}]$. We have cropped away degrees 1 and 3 and the 15 largest $\lambda $ values purely for visualization purposes}{figure.caption.44}{}}
\newlabel{fig:b MSE R2@cref}{{[figure][7][]7}{[1][8][]11}}
\@writefile{toc}{\appendix }
\@writefile{toc}{\contentsline {section}{\numberline {A}Derivations}{11}{section*.63}\protected@file@percent }
\newlabel{appsec:derivations}{{A}{11}{}{section*.63}{}}
\newlabel{appsec:derivations@cref}{{[appendix][1][2147483647]A}{[1][11][]11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1}Expectation value and variance of the OLS estimator $\boldsymbol  {\hat  {\beta }}$}{11}{section*.64}\protected@file@percent }
\newlabel{subapp:beta OLS}{{A\,1}{11}{}{section*.64}{}}
\newlabel{subapp:beta OLS@cref}{{[subappendix][1][2147483647,1]A\,1}{[1][11][]11}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces MSE (upper row) and $R^2$ (lower row) for Lasso regression evaluated on the Franke test data and contour plotted as functions of polynomial degree (horizontal axes) and hyperparameter $\lambda $ (vertical axes). We have used 11 evenly spaced degrees in the interval $[1, 41]$ and 30 logarithmically spaced values for $\lambda $ in the interval $[10^{-5},10^{-1}]$, but in the right column we have cropped away degrees 1 and 5 and the 20 largest $\lambda $ values.}}{12}{figure.caption.46}\protected@file@percent }
\newlabel{fig:c MSE R2}{{8}{12}{MSE (upper row) and $R^2$ (lower row) for Lasso regression evaluated on the Franke test data and contour plotted as functions of polynomial degree (horizontal axes) and hyperparameter $\lambda $ (vertical axes). We have used 11 evenly spaced degrees in the interval $[1, 41]$ and 30 logarithmically spaced values for $\lambda $ in the interval $[10^{-5},10^{-1}]$, but in the right column we have cropped away degrees 1 and 5 and the 20 largest $\lambda $ values}{figure.caption.46}{}}
\newlabel{fig:c MSE R2@cref}{{[figure][8][]8}{[1][8][]12}}
\newlabel{appeq:expect beta OLS}{{{A5}}{12}{}{AMS.66}{}}
\newlabel{appeq:expect beta OLS@cref}{{[equation][2147483647][]{A5}}{[1][12][]12}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces MSE evaluated on the Franke function training set (blue) and test set (red) for 21 evenly spaced polynomial degrees in the interval $[1, 81]$.}}{13}{figure.caption.48}\protected@file@percent }
\newlabel{fig:e train test}{{9}{13}{MSE evaluated on the Franke function training set (blue) and test set (red) for 21 evenly spaced polynomial degrees in the interval $[1, 81]$}{figure.caption.48}{}}
\newlabel{fig:e train test@cref}{{[figure][9][]9}{[1][8][]13}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces MSE (orange), bias (blue) and variance (pink) for OLS regression evaluated on Franke test dta and plotted as functions of polynomial degree (upper left), number of data points (upper right), number of bootstrap steps (lower left) and test size fraction (lower right). When these are not varied we have used degree 5, 50 data points in one direction (i.e. $2\:500$ in the grid), 100 bootstrap steps and a test size fraction of 0.2. Note that some axes are scaled logarithmically and some have a linear scale.}}{13}{figure.caption.49}\protected@file@percent }
\newlabel{fig:e bias variance}{{10}{13}{MSE (orange), bias (blue) and variance (pink) for OLS regression evaluated on Franke test dta and plotted as functions of polynomial degree (upper left), number of data points (upper right), number of bootstrap steps (lower left) and test size fraction (lower right). When these are not varied we have used degree 5, 50 data points in one direction (i.e. $2\:500$ in the grid), 100 bootstrap steps and a test size fraction of 0.2. Note that some axes are scaled logarithmically and some have a linear scale}{figure.caption.49}{}}
\newlabel{fig:e bias variance@cref}{{[figure][10][]10}{[1][8][]13}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces MSE evaluated on Franke test data as function of polynomial degrees in the interval $[1, 15]$ and number of folds $k\in [5, 15]$ used in cross-validation (contour plot on the left). On the right we have plotted the MSE purely as function of complexity when using the bootstrap resampling method with 100 steps.}}{14}{figure.caption.52}\protected@file@percent }
\newlabel{fig:f kfold vs bootstrap}{{11}{14}{MSE evaluated on Franke test data as function of polynomial degrees in the interval $[1, 15]$ and number of folds $k\in [5, 15]$ used in cross-validation (contour plot on the left). On the right we have plotted the MSE purely as function of complexity when using the bootstrap resampling method with 100 steps}{figure.caption.52}{}}
\newlabel{fig:f kfold vs bootstrap@cref}{{[figure][11][]11}{[1][8][]14}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces MSE evaluated on cosmological test data as function of polynomial degrees in the interval $[1, 31]$ and number of folds $k\in [5, 15]$ used in cross-validation (contour plot on the left). On the right we have plotted the MSE purely as function of complexity when using the bootstrap resampling method with 100 steps.}}{14}{figure.caption.53}\protected@file@percent }
\newlabel{fig:g kfold vs bootstrap}{{12}{14}{MSE evaluated on cosmological test data as function of polynomial degrees in the interval $[1, 31]$ and number of folds $k\in [5, 15]$ used in cross-validation (contour plot on the left). On the right we have plotted the MSE purely as function of complexity when using the bootstrap resampling method with 100 steps}{figure.caption.53}{}}
\newlabel{fig:g kfold vs bootstrap@cref}{{[figure][12][]12}{[1][8][]14}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces MSE evaluated on cosmological test data as function of polynomial degrees in the interval $[1, 31]$ and number of folds $k\in [5, 15]$ used in cross-validation, for Ridge (upper row) and Lasso (lower row) regression. We have used $\lambda =0.001$ and $\lambda =0.5$ in the left and right columns, respectively.}}{15}{figure.caption.55}\protected@file@percent }
\newlabel{fig:g Ridge Lasso}{{13}{15}{MSE evaluated on cosmological test data as function of polynomial degrees in the interval $[1, 31]$ and number of folds $k\in [5, 15]$ used in cross-validation, for Ridge (upper row) and Lasso (lower row) regression. We have used $\lambda =0.001$ and $\lambda =0.5$ in the left and right columns, respectively}{figure.caption.55}{}}
\newlabel{fig:g Ridge Lasso@cref}{{[figure][13][]13}{[1][8][]15}}
\newlabel{appeq:var beta OLS}{{{A6}}{15}{}{AMS.68}{}}
\newlabel{appeq:var beta OLS@cref}{{[equation][2147483647][]{A6}}{[1][14][]15}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Contour plots of the cosmological simulation data (upper left) and the OLS fits for polynomial degrees 5 (upper right), 10 (middle left), 20 (middle right), 30 (lower left) and 50 (lower right).}}{16}{figure.caption.58}\protected@file@percent }
\newlabel{fig:density complexity}{{14}{16}{Contour plots of the cosmological simulation data (upper left) and the OLS fits for polynomial degrees 5 (upper right), 10 (middle left), 20 (middle right), 30 (lower left) and 50 (lower right)}{figure.caption.58}{}}
\newlabel{fig:density complexity@cref}{{[figure][14][]14}{[1][8][]16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2}MSE expressed in terms of model bias, model variance and noise variance}{16}{section*.69}\protected@file@percent }
\newlabel{subapp:cost}{{A\,2}{16}{}{section*.69}{}}
\newlabel{subapp:cost@cref}{{[subappendix][2][2147483647,1]A\,2}{[1][15][]16}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Contour plots of the cosmological simulation data (upper left) and OLS (upper right), Ridge (lower left) and Lasso (lower right) regression fits for polynomial degree 31 and $k=15$ number of folds in cross-validation. For Ridge regression we used \colorbox {magenta}{lambda} and for Lasso we used \colorbox {magenta}{lambda}.}}{17}{figure.caption.60}\protected@file@percent }
\newlabel{fig:density OLS Ridge Lasso}{{15}{17}{Contour plots of the cosmological simulation data (upper left) and OLS (upper right), Ridge (lower left) and Lasso (lower right) regression fits for polynomial degree 31 and $k=15$ number of folds in cross-validation. For Ridge regression we used \colorbox {magenta}{lambda} and for Lasso we used \colorbox {magenta}{lambda}}{figure.caption.60}{}}
\newlabel{fig:density OLS Ridge Lasso@cref}{{[figure][15][]15}{[1][9][]17}}
\bibstyle{apsrev4-1}
\citation{REVTEX41Control}
\citation{apsrev41Control}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces The coefficients $\beta $ for the Franke function fit estimated using unscaled (blue) and scaled (red) data, plotted for polynomial degrees in the interval $[1,6]$.}}{18}{figure.caption.73}\protected@file@percent }
\newlabel{appfig:a beta scaled vs raw}{{16}{18}{The coefficients $\beta $ for the Franke function fit estimated using unscaled (blue) and scaled (red) data, plotted for polynomial degrees in the interval $[1,6]$}{figure.caption.73}{}}
\newlabel{appfig:a beta scaled vs raw@cref}{{[figure][16][2147483647]16}{[1][18][]18}}
\newlabel{appeq:cost OLS}{{{A7}}{18}{}{AMS.71}{}}
\newlabel{appeq:cost OLS@cref}{{[equation][2147483647][]{A7}}{[1][18][]18}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Additional Figures}{18}{section*.72}\protected@file@percent }
\newlabel{appsec:figures}{{B}{18}{}{section*.72}{}}
\newlabel{appsec:figures@cref}{{[appendix][2][2147483647]B}{[1][18][]18}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces MSE (upper row) and $R^2$ (lower row) for Ridge regression evaluated on the cosmological test data and contour plotted as functions of polynomial degree (horizontal axes) and hyperparameter $\lambda $ (vertical axes). We have used 11 evenly spaced degrees in the interval $[1, 31]$ and 11 logarithmically spaced values for $\lambda $ in the interval $[10^{-5},10^{-1}]$. In the right column we have cropped away degrees smaller than 19 purely for visualization purposes.}}{19}{figure.caption.74}\protected@file@percent }
\newlabel{appfig:g MSE R2 Ridge}{{17}{19}{MSE (upper row) and $R^2$ (lower row) for Ridge regression evaluated on the cosmological test data and contour plotted as functions of polynomial degree (horizontal axes) and hyperparameter $\lambda $ (vertical axes). We have used 11 evenly spaced degrees in the interval $[1, 31]$ and 11 logarithmically spaced values for $\lambda $ in the interval $[10^{-5},10^{-1}]$. In the right column we have cropped away degrees smaller than 19 purely for visualization purposes}{figure.caption.74}{}}
\newlabel{appfig:g MSE R2 Ridge@cref}{{[figure][17][2147483647]17}{[1][18][]19}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces MSE (upper row) and $R^2$ (lower row) for Lasso regression evaluated on the cosmological test data and contour plotted as functions of polynomial degree (horizontal axes) and hyperparameter $\lambda $ (vertical axes). We have used 11 evenly spaced degrees in the interval $[1, 31]$ and 11 logarithmically spaced values for $\lambda $ in the interval $[10^{-5},10^{-1}]$, but in the right column we have cropped away degrees smaller than 19 and the 3 largest $\lambda $ values.}}{20}{figure.caption.75}\protected@file@percent }
\newlabel{appfig:g MSE R2 Lasso}{{18}{20}{MSE (upper row) and $R^2$ (lower row) for Lasso regression evaluated on the cosmological test data and contour plotted as functions of polynomial degree (horizontal axes) and hyperparameter $\lambda $ (vertical axes). We have used 11 evenly spaced degrees in the interval $[1, 31]$ and 11 logarithmically spaced values for $\lambda $ in the interval $[10^{-5},10^{-1}]$, but in the right column we have cropped away degrees smaller than 19 and the 3 largest $\lambda $ values}{figure.caption.75}{}}
\newlabel{appfig:g MSE R2 Lasso@cref}{{[figure][18][2147483647]18}{[1][18][]20}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces MSE evaluated on Franke test data as function of polynomial degrees in the interval $[1, 15]$ and number of folds $k\in [5, 15]$ used in cross-validation, for Ridge (upper row) and Lasso (lower row) regression. We have used $\lambda =0.001$ and $\lambda =0.5$ in the left and right columns, respectively.}}{21}{figure.caption.76}\protected@file@percent }
\newlabel{appfig:f Ridge Lasso}{{19}{21}{MSE evaluated on Franke test data as function of polynomial degrees in the interval $[1, 15]$ and number of folds $k\in [5, 15]$ used in cross-validation, for Ridge (upper row) and Lasso (lower row) regression. We have used $\lambda =0.001$ and $\lambda =0.5$ in the left and right columns, respectively}{figure.caption.76}{}}
\newlabel{appfig:f Ridge Lasso@cref}{{[figure][19][2147483647]19}{[1][18][]21}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Surface plots of the cosmological simulation data (left) and the OLS fit for polynomial degree 30 (right).}}{22}{figure.caption.77}\protected@file@percent }
\newlabel{appfig:density surf}{{20}{22}{Surface plots of the cosmological simulation data (left) and the OLS fit for polynomial degree 30 (right)}{figure.caption.77}{}}
\newlabel{appfig:density surf@cref}{{[figure][20][2147483647]20}{[1][18][]22}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Contour plots of the cosmological simulation data (upper left) and the OLS fits for the entire data (upper right), 4 separate pieces stitched back together (lower left) and 100 separate pieces stitched back together (lower right). We have used a polynomial degree of 30 for all fits.}}{23}{figure.caption.78}\protected@file@percent }
\newlabel{appfig:density pieces}{{21}{23}{Contour plots of the cosmological simulation data (upper left) and the OLS fits for the entire data (upper right), 4 separate pieces stitched back together (lower left) and 100 separate pieces stitched back together (lower right). We have used a polynomial degree of 30 for all fits}{figure.caption.78}{}}
\newlabel{appfig:density pieces@cref}{{[figure][21][2147483647]21}{[1][18][]23}}
\newlabel{LastPage}{{}{23}{}{}{}}
\gdef \@abspage@last{23}
