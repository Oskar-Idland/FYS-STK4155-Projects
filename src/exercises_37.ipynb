{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1>\n",
    "FYS-STK4155 - Exercises Week 37\n",
    "</div>\n",
    "<hr style=\"border:2px solid gray\"> </hr>\n",
    "<div align=\"center\"><h3>\n",
    "Håvard Skåli\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the error is normal distributed as $\\boldsymbol{\\varepsilon}\\sim N(0,\\sigma^2)$ we know that the expectation value and variance of the $i$'th element of $\\boldsymbol{\\varepsilon}$ is $\\mathbb{E}(\\varepsilon_i)=0$ and $\\textrm{Var}(\\varepsilon_i)=\\sigma^2$. Thus, since we approximate $f(\\mathbf{x})$ with $\\mathbf{\\tilde{y}}=\\mathbf{X}\\boldsymbol{\\beta}$ the expectation value of $\\mathbf{y}$ becomes \n",
    "$$\n",
    "\\mathbb{E}(\\mathbf{y})=\\mathbb{E}(\\mathbf{X}\\boldsymbol{\\beta}) + \\mathbb{E}(\\boldsymbol{\\varepsilon}) = \\mathbb{E}(\\mathbf{X}\\boldsymbol{\\beta}),\n",
    "$$\n",
    "which gives us the expectation value of $\\mathbf{y}$ for a given element $i$:\n",
    "$$\n",
    "\\mathbb{E}(y_i) = \\mathbb{E}\\left(\\sum_jX_{ij}\\beta_j\\right) = \\sum_jX_{ij}\\beta_j = \\mathbf{X}_{i,*}\\boldsymbol{\\beta}.\n",
    "$$\n",
    "Here I have used that the sum $\\sum_jX_{ij}\\beta_j$ is known to be the value of $\\tilde{y}_i$, hence its expectation value is itself. Moreover, we can similarly find the variance of $\\mathbf{y}$ for a given element $i$ by using that the aforementioned sum is known to be $\\tilde{y}_i$ for all $i$, i.e. $\\textrm{Var}(\\mathbf{X}_{i,*}\\boldsymbol{\\beta})=0$. Thus, we have\n",
    "$$\n",
    "\\textrm{Var}(y_i) = \\textrm{Var}(\\mathbf{X}_{i,*}\\boldsymbol{\\beta}) + \\textrm{Var}(\\varepsilon_i) = \\sigma^2,\n",
    "$$\n",
    "and consequently\n",
    "$$\n",
    "y_i\\sim N(\\mathbf{X}_{i,*}\\boldsymbol{\\beta},\\sigma^2),\n",
    "$$\n",
    "just like we wanted to show. Now, using that the optimal parameters in OLS are given by $\\boldsymbol{\\beta}_\\textrm{OLS}=\\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^\\textrm{T}\\mathbf{y}$, their expectation values become\n",
    "$$\n",
    "\\mathbb{E}(\\boldsymbol{\\beta}_\\textrm{OLS})=\\mathbb{E}\\left\\{ \\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^\\textrm{T}\\mathbf{y} \\right\\}\n",
    "= \\mathbb{E}\\left\\{\\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^\\textrm{T}\\right\\} \\mathbb{E}(\\mathbf{y}) \n",
    "= \\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^\\textrm{T}\\mathbf{X}\\boldsymbol{\\beta} = \\boldsymbol{\\beta}. \n",
    "$$\n",
    "Furthermore, if $x$ and $y$ are two independent variables, the variance of their product is given by\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\textrm{Var}(xy) &= \\mathbb{E}(x^2y^2)-(\\mathbb{E}(xy))^2, \\\\\n",
    "&= \\mathbb{E}(x^2)\\mathbb{E}(y^2) - \\left(\\mathbb{E}(x)\\right)^2\\left(\\mathbb{E}(y)\\right)^2,\n",
    "\\\\\n",
    "&= \\left[\\mathbb{E}(x^2) - \\left(\\mathbb{E}(x)\\right)^2 + \\left(\\mathbb{E}(x)\\right)^2\\right]\\left[\\mathbb{E}(y^2) - \\left(\\mathbb{E}(y)\\right)^2 + \\left(\\mathbb{E}(y)\\right)^2\\right] - \\mathbb{E}(x^2)\\mathbb{E}(y^2),\n",
    "\\\\\n",
    "&= \\left[\\textrm{Var}(x) + \\left(\\mathbb{E}(x)\\right)^2\\right]\\left[\\textrm{Var}(y) + \\left(\\mathbb{E}(y)\\right)^2\\right] - \\mathbb{E}(x^2)\\mathbb{E}(y^2), \\\\\n",
    "&= \\textrm{Var}(x)\\textrm{Var}(y) + \\textrm{Var}(x)(\\mathbb{E}(y))^2 + \\textrm{Var}(y)(\\mathbb{E}(x))^2 +\\mathbb{E}(x^2)\\mathbb{E}(y^2)- \\mathbb{E}(x^2)\\mathbb{E}(y^2),\n",
    "\\\\\n",
    "&= \\textrm{Var}(x)\\textrm{Var}(y) + \\textrm{Var}(x)(\\mathbb{E}(y))^2 + \\textrm{Var}(y)(\\mathbb{E}(x))^2,\n",
    "\\end{aligned}\n",
    "$$\n",
    "so if we now set $x=\\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^\\textrm{T}$ and $y=\\mathbf{y}$ we find that the variance of $\\boldsymbol{\\beta}_\\textrm{OLS}$ is\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\textrm{Var}(\\boldsymbol{\\beta}_\\textrm{OLS}) \n",
    "&= \\underbrace{\\textrm{Var}\\left[\\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^\\textrm{T}\\right]\\textrm{Var}(\\mathbf{y})}_0 \n",
    "+ \\underbrace{\\textrm{Var}\\left[\\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^\\textrm{T}\\right](\\mathbb{E}(\\mathbf{y}))^2}_{0} \n",
    "+ \\textrm{Var}(\\mathbf{y})\\left(\\mathbb{E}\\left[\\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^\\textrm{T}\\right]\\right)^2,\n",
    "\\\\\n",
    "&= \\sigma^2\\left[\\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^\\textrm{T}\\right]^2,\n",
    "\\\\\n",
    "&= \\sigma^2\\left[\\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^\\textrm{T}\\right]\\left[\\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^\\textrm{T}\\right]^\\textrm{T},\n",
    "\\\\\n",
    "&= \\sigma^2\\left[\\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^\\textrm{T}\\right]\\left[\\mathbf{X}\\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}\\right)^{-1}\\right],\n",
    "\\\\\n",
    "&= \\sigma^2\\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}\\right)^{-1}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "Here I have used that the transpose of $\\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}\\right)^{-1}$ is itself since it is square and symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same arguments as in the previous exercise one can easily show that $\\mathbb{E}(\\mathbf{y})=\\mathbb{E}(\\mathbf{X}\\boldsymbol{\\beta})=\\mathbf{X}\\boldsymbol{\\beta}$ and $\\textrm{Var}(\\mathbf{y})=\\textrm{Var}(\\boldsymbol{\\varepsilon})=\\sigma^2$ for Ridge regression as well, just like for OLS. Following the same approach as above we then have\n",
    "$$\n",
    "\\mathbb{E}(\\boldsymbol{\\beta}_\\textrm{Ridge})\n",
    "= \\mathbb{E}\\left\\{ \\left(\\mathbf{X}^\\textrm{T}\\mathbf{X} + \\lambda\\mathbf{I}_{p\\times p}\\right)^{-1}\\mathbf{X}^\\textrm{T}\\mathbf{y} \\right\\}\n",
    "= \\mathbb{E}\\left\\{\\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}+ \\lambda\\mathbf{I}_{p\\times p}\\right)^{-1}\\mathbf{X}^\\textrm{T}\\right\\} \\mathbb{E}(\\mathbf{y}) \n",
    "= \\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}+ \\lambda\\mathbf{I}_{p\\times p}\\right)^{-1}\\mathbf{X}^\\textrm{T}\\mathbf{X}\\boldsymbol{\\beta}, \n",
    "$$\n",
    "like we wanted to show. Furthermore, the variance becomes\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\textrm{Var}(\\boldsymbol{\\beta}_\\textrm{Ridge}) \n",
    "&= \\underbrace{\\textrm{Var}\\left[\\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}+ \\lambda\\mathbf{I}_{p\\times p}\\right)^{-1}\\mathbf{X}^\\textrm{T}\\right]\\textrm{Var}(\\mathbf{y})}_0 \n",
    "+ \\underbrace{\\textrm{Var}\\left[\\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}+ \\lambda\\mathbf{I}_{p\\times p}\\right)^{-1}\\mathbf{X}^\\textrm{T}\\right](\\mathbb{E}(\\mathbf{y}))^2}_{0} \n",
    "+ \\textrm{Var}(\\mathbf{y})\\left(\\mathbb{E}\\left[\\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}+ \\lambda\\mathbf{I}_{p\\times p}\\right)^{-1}\\mathbf{X}^\\textrm{T}\\right]\\right)^2,\n",
    "\\\\\n",
    "&= \\sigma^2\\left[\\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}+ \\lambda\\mathbf{I}_{p\\times p}\\right)^{-1}\\mathbf{X}^\\textrm{T}\\right]^2,\n",
    "\\\\\n",
    "&= \\sigma^2\\left[\\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}+ \\lambda\\mathbf{I}_{p\\times p}\\right)^{-1}\\mathbf{X}^\\textrm{T}\\right]\\left[\\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}+ \\lambda\\mathbf{I}_{p\\times p}\\right)^{-1}\\mathbf{X}^\\textrm{T}\\right]^\\textrm{T},\n",
    "\\\\\n",
    "&= \\sigma^2\\left[\\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}+ \\lambda\\mathbf{I}_{p\\times p}\\right)^{-1}\\mathbf{X}^\\textrm{T}\\right]\\left[\\mathbf{X}\\left\\{\\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}+ \\lambda\\mathbf{I}_{p\\times p}\\right)^{-1}\\right\\}^\\textrm{T}\\right],\n",
    "\\\\\n",
    "&= \\sigma^2\\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}+ \\lambda\\mathbf{I}_{p\\times p}\\right)^{-1}\\mathbf{X}^\\textrm{T}\\mathbf{X}\\left\\{\\left(\\mathbf{X}^\\textrm{T}\\mathbf{X}+ \\lambda\\mathbf{I}_{p\\times p}\\right)^{-1}\\right\\}^\\textrm{T},\n",
    "\\end{aligned}\n",
    "$$\n",
    "which is the desired result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
